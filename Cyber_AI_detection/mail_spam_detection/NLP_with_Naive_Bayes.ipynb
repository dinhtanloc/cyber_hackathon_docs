{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuoj7m2L7J1O"
      },
      "source": [
        "The phases that chracterzie NPL are\n",
        "\n",
        "1.   Identification of the words (tokens) constituting the language\n",
        "\n",
        "1.   Analysis of the strcuture of the text\n",
        "2.   Identification of the relationships between words(in paragrahs, sentences and so on)\n",
        "2.   Semantic analysis of the text\n",
        "\n",
        "\n",
        "The best known python libraries for NLP is Natural Language Toolkit (NLTK)\n",
        "\n",
        "The data will be treated with the bag of words (BoW)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FUPQlwN68LVW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ThKeSp2J8cEn"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsq9PBEI8vv3",
        "outputId": "6f43f2f9-1429-4095-db57-905383dc80e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will ü b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5572</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5573</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5574 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      type                                               text\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...    ...                                                ...\n",
              "5569  spam  This is the 2nd time we have tried 2 contact u...\n",
              "5570   ham               Will ü b going to esplanade fr home?\n",
              "5571   ham  Pity, * was in mood for that. So...any other s...\n",
              "5572   ham  The guy did some bitching but I acted like i'd...\n",
              "5573   ham                         Rofl. Its true to its name\n",
              "\n",
              "[5574 rows x 2 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from defs import get_tokens\n",
        "from defs import get_lemmas\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "sms = pd.read_csv('data/sms_spam_no_header.csv', sep=',', names = [\"type\", \"text\"])\n",
        "text_train, text_test, type_train, type_test = train_test_split(sms['text'], sms['type'], test_size = 0.3)\n",
        "#bow stands for \"Bag of Words\"\n",
        "sms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "bow = CountVectorizer(analyzer=get_lemmas).fit(text_train)\n",
        "sms_bow = bow.transform(text_train)\n",
        "tfidf = TfidfTransformer().fit(sms_bow)\n",
        "sms_tfidf = tfidf.transform(sms_bow)\n",
        "spam_detector = MultinomialNB().fit(sms_tfidf, type_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34uz2G4j-UYq",
        "outputId": "e1aa3d47-cf05-46bb-f8b0-dd1f483b0feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted:  ham\n",
            "expected:  ham\n"
          ]
        }
      ],
      "source": [
        "msg = sms['text'][25]\n",
        "msg_bow = bow.transform([msg])\n",
        "msg_tfidf = tfidf.transform(msg_bow)\n",
        "\n",
        "print('predicted: ', spam_detector.predict(msg_tfidf)[0])\n",
        "print('expected: ', sms.type[25])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7UczIUj-2f7",
        "outputId": "5b1902b4-c5aa-474e-9749-364865f29d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted:  spam\n",
            "expected:  spam\n"
          ]
        }
      ],
      "source": [
        "msg = sms['text'][11]\n",
        "msg_bow = bow.transform([msg])\n",
        "msg_tfidf = tfidf.transform(msg_bow)\n",
        "\n",
        "print('predicted: ', spam_detector.predict(msg_tfidf)[0])\n",
        "print('expected: ', sms.type[11])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqiyRfvi--QE",
        "outputId": "965c1cfe-dc3c-4f56-f52f-968a5b2d3320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy 0.7931299666752115\n"
          ]
        }
      ],
      "source": [
        "predictions = spam_detector.predict(sms_tfidf)\n",
        "print('accuracy', accuracy_score(sms['type'][:len(predictions)], predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfwY0Z-y_WOv",
        "outputId": "1d632c10-32d7-4481-c475-9ddcdb9da2f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.87      0.90      0.88      3382\n",
            "        spam       0.14      0.10      0.12       519\n",
            "\n",
            "    accuracy                           0.79      3901\n",
            "   macro avg       0.50      0.50      0.50      3901\n",
            "weighted avg       0.77      0.79      0.78      3901\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(sms['type'][:len(predictions)], predictions))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "NLP with Naive Bayes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
